{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7374edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I read in my txt version of the textbook dictionary using utf-8 encoding and I'm able to read it in as a list of\n",
    "# lowercased russian words their page number and definition.\n",
    "nti = open('./textbook_vocab_data/novice_to_intermediate.txt', encoding= 'utf8')\n",
    "nti = nti.read()\n",
    "nti # very messy ugh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "nti_txt = nti.split('\\n') # I turn the file into a list, splitting on linebreaks to divide most of the vocab items\n",
    "nti_txt_low = [x.lower() for x in nti_txt] # Stress marking was encoded with capital letters, so I lower everything\n",
    "ntiplay = nti_txt_low[:30] # let't take a look at some of the entries and make it a play set\n",
    "ntiplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85012757",
   "metadata": {},
   "source": [
    "It was ultimately easier to use a less coding-intensive solution to end up with a txt file that I can read into this environment. I'm going to try to keep the russian vocabulary item and its english definition so I'll try to get rid of some of the formatting weirdness with regular expressions. I will have to be careful with these extended definitions that continue on to multiple lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8dccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, We need to get rid of any page numbers and extraneous white spaces\n",
    "ntiplay = [re.sub('(\\s\\d+,|\\s\\d+\\s|\\|)', ' ', x) for x in ntiplay]\n",
    "ntiplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make this into a dataframe and see what we can do\n",
    "df_ntiplay=pd.DataFrame(ntiplay,columns=['Entry'])\n",
    "df_ntiplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntiplay.join(df_ntiplay['Entry'].str.split('–', 1, expand=True).rename(columns={0:'Russian', 1:'English'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ba914",
   "metadata": {},
   "outputs": [],
   "source": [
    "nti_sub = [re.sub('(\\s\\d+,|\\s\\d+\\s|\\|)', ' ', x) for x in nti_txt_low]\n",
    "df_nti=pd.DataFrame(nti_sub,columns=['Entry'])\n",
    "df_nti=df_nti.join(df_nti['Entry'].str.split('–', 1, expand=True).rename(columns={0:'Russian', 1:'English'}))\n",
    "df_nti=df_nti[['Russian','English']]\n",
    "df_nti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nti['Level'] = 'Int'\n",
    "df_nti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db887c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ita = open('./textbook_vocab_data/intermediate_to_advanced.txt', encoding= 'utf8')\n",
    "ita = ita.read()\n",
    "ita # very messy ugh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ece37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_txt = ita.split('\\n') # I turn the file into a list, splitting on linebreaks to divide most of the vocab items\n",
    "ita_txt_low = [x.lower() for x in ita_txt] # Stress marking was encoded with capital letters, so I lower everything\n",
    "itaplay = ita_txt_low[:30] # let't take a look at some of the entries and make it a play set\n",
    "itaplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_sub = [re.sub('(\\s\\d+,|\\s\\d+\\s|\\|)', ' ', x) for x in ita_txt_low]\n",
    "df_ita=pd.DataFrame(ita_sub,columns=['Entry'])\n",
    "df_ita=df_ita.join(df_ita['Entry'].str.split('-', 1, expand=True).rename(columns={0:'Russian', 1:'English'}))\n",
    "df_ita=df_ita[['Russian','English']]\n",
    "df_ita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ita['Level'] = 'Adv'\n",
    "df_ita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e808a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = pd.concat([df_nti,df_ita], axis=0)\n",
    "df_vocab = df_vocab.sort_values(by='Russian')\n",
    "df_vocab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab=df_vocab.dropna()\n",
    "df_vocab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad12933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab.sample(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf26b71",
   "metadata": {},
   "source": [
    "There are so many other considerations that have to be made for pulling some of these items out. I have to think more about what needs to be removed because there is a LOT of grammatical information coming in tandem with some of these items. For example the preposition \"без\" is being given with the case it governs (genitive). The verbs of motion \"бежать ~ бегать\" have present and past conjugations and imperatives along with the definition. Splitting on the new line character to create a list was a first good step, but I might have to go low tech and try to figure out which addtional info needs to be removed... frustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fffaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm running into more \n",
    "russ_words = pd.read_csv(\"./textbook_vocab_data/russian-word-list-total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c6cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
